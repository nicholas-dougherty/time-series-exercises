{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d773f5-f13d-45f4-82ff-2afa1a57a4fb",
   "metadata": {},
   "source": [
    "PRACTICAL TIME SERIES ANALYSIS\n",
    "Notes from the O'Reilly Publication\n",
    "“Practical Time Series Analysis by Aileen Nielsen (O’Reilly). Copyright 2020 Aileen Nielsen, 978-1-492-04165-8.”\n",
    "aileen.a.nielsen@gmail.com\n",
    "***\n",
    "\n",
    "\"For better or worse, sensors and tracking mechanisms are everywhere, and as a result there are unprecedented amounts of high-quality time series data available. Time series are uniquely interesting because they can address questions of causality, trends, and the likelihood of future outcomes.\"\n",
    "\n",
    "Henceforth, unless noted otherwise, everything will be imaginarily in quotes. Copying straight from the source. \n",
    "\n",
    "Time series is an important aspect of data analysis but one that is not found in the standard data science toolkit. This is unfortunate both because time series data is increasingly available and also because it answers questions that cross-sectional data cannot. An analyst who does not know fundamental time series analysis is not making the most of their data.\n",
    "\n",
    "Time series is an interesting topic with quirky data concerns. Problems associated with data leakage, lookahead, and causality are particularly fun from a time series perspective, as are many techniques that apply uniquely to data ordered along some kind of time axis.\n",
    "\n",
    "***\n",
    "\n",
    "# Chapter 1: Time Series: An Overview and a Quick History\n",
    "\n",
    "As continuous monitoring and data collection become more common, the need for competent time series analysis with both statistical and machine learning techniques will increase. Indeed, the most promising new models combine both of these methodologies.\n",
    "\n",
    "Time series analysis is the endeavor of extracting meaningful summary and statistical information from points arranged in chronological order. It is done to diagnose past behavior as well as to predict future behavior.\n",
    "\n",
    "Time series analysis often comes down to the question of causality: how did the past influence the future? At times, such questions (and their answers) are treated strictly within their discipline rather than as part of the general discipline of time series analysis.\n",
    "\n",
    "Likewise, individualized healthcare using time series analysis remains a young and challenging field because it can be quite difficult to create data sets that are consistent over time. Even for small case-study-based research, maintaining both contact with and participation from a group of individuals is excruciatingly difficult and expensive. When such studies are conducted for long periods of time, they tend to become canonical in their fields—and repeatedly, or even excessively researched—because their data can address important questions despite the challenges of funding and management.\n",
    "\n",
    "Indicators of production and efficiency in markets have long provided interesting data to study from a time series analysis. Most interesting and urgent has been the question of forecasting future economic states based on the past. Such forecasts aren’t merely useful for making money—they also help promote prosperity and avert social catastrophes.\n",
    "\n",
    "Nowadays, the United States and most other nations have thousands of government researchers and recordkeepers whose jobs are to record data as accurately as possible and make it available to the public (see Figure 1-4). This practice has proven invaluable to economic growth and the avoidance of economic catastrophe and painful __boom and bust cycles__.\n",
    "\n",
    "One of the pioneers of mechanical trading, or time series forecasting via algorithm, was Richard Dennis. Dennis was a self-made millionaire who famously turned ordinary people, called the Turtles, into star traders by teaching them a few select rules about how and when to trade. These rules were developed in the 1970s and 1980s and mirrored the “AI” thinking of the 1980s, in which heuristics still strongly ruled the paradigm of how to build intelligent machines to work in the real world.\n",
    "\n",
    "Since then many “mechanical” traders have adapted these rules, which as a result have become less profitable in a crowded automated market. Mechanical traders continue to grow in number and wealth, they are continually in search of the next best thing because there is so much competition.\n",
    "\n",
    "####  Time Series Analysis Takes Off\n",
    "\n",
    "George Box, a pioneering statistician who helped develop a popular time series model, was a great pragmatist. He famously said, “All models are wrong, but some are useful.”\n",
    "\n",
    "Box made this statement in response to a common attitude that proper time series modeling was a matter of finding the best model to fit the data. As he explained, the idea that any model can describe the real world is very unlikely. Box made this pronouncement in 1978, which seems bizarrely late into the history of a field as important as time series analysis, but in fact the formal discipline was surprisingly young.\n",
    "\n",
    "More recently, practical uses for time series analysis and machine learning emerged as early as the 1980s, and included a wide variety of scenarios:\n",
    "\n",
    "Computer security specialists proposed anomaly detection as a method of identifying hackers/intrusions.\n",
    "Dynamic time warping, one of the dominant methods for “measuring” the similarity of time series, came into use because the computing power would finally allow reasonably fast computation of “distances,” say between different audio recordings.\n",
    "Recursive neural networks were invented and shown to be useful for extracting patterns from corrupted data.\n",
    "Time series analysis and forecasting have yet to reach their golden period, and, to date, time series analysis remains dominated by traditional statistical methods as well as simpler machine learning techniques, such as ensembles of trees and linear fits. We are still waiting for a great leap forward for predicting the future.\n",
    "***\n",
    "# Chapter 2. Finding and Wrangling Time Series Data\n",
    "\n",
    "In this chapter we discuss problems that might arise while you are preprocessing time series data. Some of these problems will be familiar to experienced data analysts, but there are specific difficulties posed by timestamps. __As with any data analysis task, cleaning and properly processing data is often the most important step of a timestamp pipeline. Fancy techniques can’t fix messy data.__\n",
    "\n",
    "Most data analysts will need to find, align, scrub, and smooth their own data either to learn time series analysis or to do meaningful work in their organizations. As you prepare data, you’ll need to do a variety of tasks, from joining disparate columns to resampling irregular or missing data to aligning time series with different time axes. This chapter helps you along the path to an interesting and properly prepared time series data set.\n",
    "\n",
    "### Prepared Data Sets\n",
    "\n",
    "The best way to learn an analytical or modeling technique is to run through it on a variety of data sets and see both how to apply it and whether it helps you reach a concrete goal.\n",
    "\n",
    "Notice that for the purpose of thinking about signs as time series, it doesn’t matter what the unit of time is; the point is the sequencing rather than the exact time. In that case, all you would care about is the ordering of the event, and whether you could assume or confirm from reading the data description that the measurements were taken at regular intervals.\n",
    "\n",
    "UNIVARIATE VERSUS MULTIVARIATE TIME SERIES\n",
    "\n",
    "The data sets we have looked at so far are univariate time series; that is, they have just one variable measured against time.\n",
    "\n",
    "Multivariate time series are series with multiple variables measured at each timestamp. They are particularly rich for analysis because often the measured variables are interrelated and show temporal dependencies between one another. We will encounter multivariate time series data later.\n",
    "\n",
    "It is great to work on difficult problems, but it is not a good idea to learn on such problems.\n",
    "\n",
    "### Retrofitting a Time Series Data Collection from a Collection of Tables\n",
    "\n",
    "The quintessential example of a found time series is one extracted from state-type and event-type data stored in a SQL database. This is also the most relevant example because so much data continues to be stored in traditional structured SQL databases.\n",
    "\n",
    "WHAT IS A LOOKAHEAD?\n",
    "\n",
    "The term lookahead is used in time series analysis to denote any knowledge of the future. You shouldn’t have such knowledge when designing, training, or evaluating a model. A lookahead is a way, through data, to find out something about the future earlier than you ought to know it.\n",
    "\n",
    "A lookahead is any way that information about what will happen in the future might propagate back in time in your modeling and affect how your model behaves earlier in time. For example, when choosing hyperparameters for a model, you might test the model at various times in your data set, then choose the best model and start at the beginning of your data to test this model. This is problematic because you chose the model for one time knowing things that would happen at a subsequent time—a lookahead.\n",
    "\n",
    "Unfortunately, there is no automated code or statistical test for a lookahead, so it is something you must be vigilant and thoughtful about.\n",
    "\n",
    "You may be surprised that we need 26 instead of 25 given the subtraction we just performed, but that was an incomplete calculation. When you work with time series data, one thing you should always ask yourself after doing this kind of subtraction is whether you should add 1 to account for the offset at the end. In other words, did you subtract the positions you wanted to count?\n",
    "\n",
    "Consider this example. Let’s say I have information for April 7th, 14th, 21st, and 28th. I want to know how many data points I should have in total. Subtracting 7 from 28 and dividing by 7 yields 21/7 or 3. However, I should obviously have four data points. I subtracted out April 7th and need to put it back in, so the proper calculation is the difference between the first and last days divided by 7, plus 1 to account for the subtracted start date.\n",
    "\n",
    "It’s a lot easier to fill in all missing weeks for all members by exploiting Pandas’ indexing functionality, rather than writing our own solution. We can generate a MultiIndex for a Pandas data frame, which will create all combinations of weeks and members—that is, a __Cartesian product__:\n",
    "\n",
    "#### PYTHON’S PANDAS\n",
    "\n",
    "Pandas is a data frame analysis package in Python that is used widely in the data science community. Its very name indicates its suitability for time series analysis: “Pandas” refers to “panel data,” which is what social scientists call time series data.\n",
    "\n",
    "Pandas is based on tables of data with row and column indices. It has SQL-like operations built in, such as group by, row selection, and key indexing. It also has time series–specific functionality, such as indexing by time period, downsampling, and time-based grouping operations.\n",
    "\n",
    "If you are unfamiliar with Pandas, I strongly recommend looking at a brief overview, such as that provided in the official documentation.\n",
    "\n",
    "To recap, these are the time-series-specific techniques we used to restructure the data:\n",
    "\n",
    "Recalibrate the resolution of our data to suit our question. Often data comes with more specific time information than we need.\n",
    "Understand how we can avoid lookahead by not using data for timestamps that produce the data’s availability.\n",
    "Record all relevant time periods even if “nothing happened.” A zero count is just as informative as any other count.\n",
    "Avoid lookahead by not using data for timestamps that produce information we shouldn’t yet know about.\n",
    "\n",
    "The better you understand your data pipeline, the less likely you are to ask the wrong questions because your timestamps don’t really mean what you think they do. You bear the ultimate responsibility for understanding the data. People who work upstream in the pipeline don’t know what you have in mind for analysis. Try to be as hands-on as possible in assessing how timestamps are generated. So, if you are analyzing data from a mobile app pipeline, download the app, trigger an event in a variety of scenarios, and see what your own data looks like. You’re likely to be surprised about how your actions were recorded after speaking to those who manage the data pipeline. It’s hard to track multiple clocks and contingencies, so most data sets will flatten the temporal realities. You need to know exactly how they do so.\n",
    "\n",
    "Reading the data as we did in the previous example, you can generate initial hypotheses about what the timestamps mean. In the preceding case, look at data for multiple users to see whether the same pattern (multiple rows with identical timestamps and improbable single meal contents) held or whether this was an anomaly.\n",
    "Using aggregate-level analyses, you can test hypotheses about what timestamps mean or probably mean. For the preceding data, there are a couple of open questions:\n",
    "Is the timestamp local or universal time?\n",
    "Does the time reflect a user action or some external constraint, such as connectivity?\n",
    "\n",
    "### Local or Universal Time?\n",
    "\n",
    "Most timestamps are stored in universal (UTC) time or in a single time zone, depending on the server’s location but independent of the user’s location. It is quite unusual to store data according to local time. However, we should consider both possibilities, because both are found in “the wild.”\n",
    "\n",
    "We form the hypothesis that if the time is a local timestamp (local to each user), we should see daily trends in the data reflecting daytime and nighttime behavior. More specifically, we should expect not to see much activity during the night when our users are sleeping.\n",
    "\n",
    "\n",
    "##### PSYCHOLOGICAL TIME DISCOUNTING\n",
    "\n",
    "Time discounting is a manifestation of a phenomenon known as psychological distance, which names our tendency to be more optimistic (and less realistic) when making estimates or assessments that are more “distant” from us. Time discounting predicts that data reported from further in the past will be biased systematically compared to data reported from more recent memory. This is distinct from the more general problem of forgetting and implies a nonrandom error. You should keep this in mind whenever you are looking at human-generated data that was entered manually but not contemporaneously with the event recorded.\n",
    "\n",
    "### Cleaning your Data\n",
    "\n",
    "Handling Missing Data\n",
    "\n",
    "Missing data is surprisingly common.\n",
    "To generalize at great peril, missing data is even more common in time series analysis than it is in cross-sectional data analysis because the burden of longitudinal sampling is particularly heavy: incomplete time series are quite common and so methods have been developed to deal with holes in what is recorded.\n",
    "\n",
    "The most common methods to address missing data in time series are:\n",
    "\n",
    "* Imputation: When we fill in missing data based on observations about the entire data set.\n",
    "* Interpolation: When we use neighboring data points to estimate the missing value. Interpolation can also be a form of imputation.\n",
    "* Deletion of affected time periods: When we choose not to use time periods that have missing data at all.\n",
    "\n",
    "#### Moving average\n",
    "\n",
    "We can also impute data with either a rolling mean or median. Known as a moving average, this is similar to a forward fill in that you are using past values to “predict” missing future values (imputation can be a form of prediction). With a moving average, however, you are using input from multiple recent times in the past.\n",
    "\n",
    "There are many situations where a moving average data imputation is a better fit for the task than a forward fill. For example, if the data is noisy, and you have reason to doubt the value of any individual data point relative to an overall mean, you should use a moving average rather than a forward fill. Forward filling could include random noise more than the “true” metric that interests you, whereas averaging can remove some of this noise.\n",
    "\n",
    "#### Interpolation\n",
    "\n",
    "Interpolation is a method of determining the values of missing data points based on geometric constraints regarding how we want the overall data to behave. For example, a linear interpolation constrains the missing data to a linear fit consistent with known neighboring points.\n",
    "\n",
    "Linear interpolation is particularly useful and interesting because it allows you to use your knowledge of how your system behaves over time. For example, if you know a system behaves in a linear fashion, you can build that knowledge in so that only linear trends will be used to impute missing data. In Bayesian speak, it allows you to inject a prior into your imputation.\n",
    "\n",
    "__Data imputation remains an important area of data science research. The more significant the decisions you are making, the more important it is to carefully consider to the potential reasons for data to be missing and the potential ramifications of your corrections__ .\n",
    "\n",
    "Here are a few cautionary tips you should keep in mind:\n",
    "\n",
    "* It is impossible to prove that data is truly missing at random, and it is unlikely that missingness is truly random in most real-world occurrences.\n",
    "* Sometimes the probability that a measurement is missing is explainable by the variables you have measured, but sometimes not. Wide data sets with many features are the best way to investigate possible explanations for patterns of missing data, but these are not the norm for time series analysis.\n",
    "* When you need to understand the uncertainty introduced by imputing values to missing data, you should run through a variety of scenarios and also speak to as many people involved in the data collection process as possible.\n",
    "* How you handle missing data should account for your downstream use of that data. You must guard carefully against lookaheads or decide how seriously a lookahead will affect the validity of your subsequent work.\n",
    "\n",
    "#### Upsampling and Downsampling \n",
    "\n",
    "Often, related time series data from different sources will not have the same sampling frequency. This is one reason, among many, that you might wish to change the sampling frequency of your data. Of course you cannot change the actual rate at which information was measured, but you can change the frequency of the timestamps in your data collection. This is called upsampling and downsampling, for increasing or decreasing the timestamp frequency, respectively.\n",
    "\n",
    "NOTE\n",
    "Downsampling is subsetting data such that the timestamps occur at a lower frequency than in the original time series. Upsampling is representing data as if it were collected more frequently than was actually the case.\n",
    "\n",
    "##### Downsampling\n",
    "\n",
    "Anytime you reduce frequency of your data, you are downsampling. This is most often done in the following cases.\n",
    "\n",
    "The original resolution of the data isn’t sensible\n",
    "\n",
    "There can be many reasons that the original granularity of the data isn’t sensible. For example, you may be measuring something too often. Suppose you have a data set where someone had measured the outside air temperature every second. Common experience dictates that this measurement is unduly frequent and likely offers very little new information relative to the additional data storage and processing burden. In fact, it’s likely the measurement error could be as large as the second-to-second air temperature variation. So, you likely don’t want to store such excessive and uninformative data. In this case—that is, for regularly sampled data—downsampling is as simple as selecting out every nth element.\n",
    "\n",
    "Focus on a particular portion of a seasonal cycle\n",
    "\n",
    "Instead of worrying about seasonal data in a time series, you might choose to create a subseries focusing on only one season. For example, we can apply downsampling to create a subseries, as in this case, where we generate a time series of January measurements out of what was originally a monthly time series. In the process, we have downsampled the data to a yearly frequency.\n",
    "\n",
    "Match against data at a lower frequency\n",
    "\n",
    "You may want to downsample data so that you can match it with other low-frequency data. In such cases you likely want to aggregate the data or downsample rather than simply dropping points. This can be something simple like a mean or a sum, or something more complicated like a weighted mean, with later values given more weight. We saw earlier in the donation data the idea of summing all donations over a single week, since it was the total amount donated that was likely to be most interesting.\n",
    "\n",
    "In contrast, for our economic data, what is most likely to be interesting is a yearly average. We use a mean instead of a rolling mean because we want to summarize the year rather than get the latest value of that year to emphasize recency. (Note the difference from data imputation.) We group by formatting the date into a string, representing its year as an example of how you can creatively exploit SQL-like operations for time series functionality:\n",
    "\n",
    "##### Upsampling\n",
    "\n",
    "Upsampling is not simply the inverse of downsampling. Downsampling makes inherent sense as something that can be done in the real world; it’s simple to decide to measure less often. In contrast, upsampling can be like trying to get something for free—that is, not taking a measurement but still somehow thinking you can get high-resolution data from infrequent measurements. To quote the author of the popular R time series package XTS:\n",
    "\n",
    "It is not possible to convert a series from a lower periodicity to a higher periodicity - e.g., weekly to daily or daily to 5 minute bars, as that would require magic.\n",
    "However, there are legitimate reasons to want to label data at a higher frequency than its default frequency. You simply need to keep in mind the data’s limitations as you do so. Remember that you are adding more time labels but not more information.\n",
    "\n",
    "Let’s discuss a few situations where upsampling can make sense.\n",
    "\n",
    "###### _Irregular time series_\n",
    "\n",
    "A very common reason to upsample is that you have an irregularly sampled time series and you want to convert it to a regularly timed one. This is a form of upsampling because you are converting all the data to a frequency that is likely higher than indicated by the lags between your data. If you are upsampling for this reason, you already know how to do it with a rolling join\n",
    "\n",
    "###### _Inputs sampled at different frequencies_\n",
    "\n",
    "Sometimes you need to upsample low-frequency information simply to carry it forward with your higher-frequency information in a model that requires your inputs to be aligned and sampled contemporaneously. You must be vigilant with respect to lookahead, but if we assume that known states are true until a new known state comes into the picture, we can safely upsample and carry our data forward. For example, suppose we know it’s (relatively) true that most new jobs start on the first of the month. We might decide we feel comfortable using the unemployment rate for a given month indicated by the jobs report for the entire month (not considering it a lookahead because we make the assumption that the unemployment rate stays steady for the month).\n",
    "\n",
    "###### _Knowledge of time series dynamics_\n",
    "\n",
    "If you have underlying knowledge of the usual temporal behavior of a variable, you may also be able to treat an upsampling problem as a missing data problem. In that case, all the techniques we’ve discussed already still apply. An interpolation is the most likely way to produce new data points, but you would need to be sure the dynamics of your system could justify your interpolation decision.\n",
    "\n",
    "As discussed earlier, upsampling and downsampling will routinely happen even in the cleanest data set because you will almost always want to compare variables of different timescales. It should also be noted that Pandas has particularly handy upsampling and downsampling functionality with the resample method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
